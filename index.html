<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script> -->
  <!-- <script type="text/javascript" src="js/hidebib.js"></script> -->
  <!-- <script src="pet_cursor.js"></script>  -->
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-7580334-2');
  </script>

  <title>Cihang Xie</title>
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
  
  <meta name="author" content="Cihang Xie">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="stylesheet" type="text/css" href="css/style.css">
  <link rel="icon" type="image/png" href="images/UCSC_icon.png">
</head>

<!-- <body onload="startPetCursor();"> -->
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:70%;vertical-align:middle">
              <p style="text-align:center">
                <name>Cihang Xie</name>
              </p>
              <p>I am an Assistant Professor of <a href="https://www.soe.ucsc.edu/departments/computer-science-and-engineering">Computer Science and Engineering</a> at <a href="https://www.ucsc.edu/">University of California, Santa Cruz</a>. My research interest lies at the intersection of computer vision and machine learning, with the goal of building human-level computer vision systems. I am particularly interested in securing model performance under distribution shifts, and developing deep representation learning with minimal supervision.
              </p>


              <p>I received my Ph.D. degree from <a href="https://www.jhu.edu/">Johns Hopkins University</a>, advised by <a href="http://www.cs.jhu.edu/~ayuille/">Bloomberg Distinguished Professor Alan Yuille</a>. I have worked as a research intern with <a href="http://kaiminghe.com/">Kaiming He</a> and <a href="https://lvdmaaten.github.io/">Laurens van der Maaten</a> at the Facebook AI Research (FAIR); <a href="https://cs.stanford.edu/~quocle/">Quoc Le</a> at the Google Brain. I receive the <a href="https://research.fb.com/fellows/xie-cihang/">2020 Facebook Fellowship</a>.
              </p>

<!--               <p style="text-align:center">
                <a href="mailto:cixie@ucsc.edu">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=X3vVZPcAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/cihangxie">Github</a>
              </p> -->
              <p style="text-align:center">
                </br>
                <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
                <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
                <a href="mailto:cixie@ucsc.edu"><i class="fa fa-envelope" style='font-size:30px'></i>&nbsp &nbsp
                <a href="https://scholar.google.com/citations?user=X3vVZPcAAAAJ&hl=en"><i class="ai ai-google-scholar ai-3x" style='font-size:28px'></i>&nbsp &nbsp
                <a href="https://github.com/cihangxie"><i class="fa fa-github" style='font-size:30px'></i>&nbsp &nbsp
                <a href="https://twitter.com/cihangxie?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @cihangxie</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/CihangXie.jpg"><img style="width:100%;max-width:100%;border-radius:50%" alt="profile photo" src="images/CihangXie.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <p>
                      <!-- <span style="background-color: #FFFF00"> -->
                      <font color="orange"><strong>Hiring</strong></font>: I am co-leading the <a href="https://ucsc-vlaa.github.io/">Vision · Learning · Assured Autonomy (VLAA) Lab</a> with <a href="https://yuyinzhou.github.io/">Professor Yuyin Zhou</a></a>. Our lab 
                      
                      <!-- has <strong>a few fully-funded Ph.D. positions for Fall 2024</strong>. In addition, we  -->
                      has
                      <strong>multiple openings for summer interns and visiting students with flexible starting times</strong>. If you are interested in these opportunities, please complete this <a href="https://ucsc-vlaa.github.io/opening.html">form</a>.</p>
                    
                    <!-- <p><font color="orange"><strong>Prospective summer interns & visiting students</strong></font>: My group has . If you are interested, please send me an email with your CV, transcript, and publications (if any).</p> -->

                    <!-- <p><font color="red"><strong>Due to the large amount of emails I receive, I may not be able to respond to each one individually.</strong></font> -->

                    <!-- <p><font color="olive"><strong>Seminar</strong></font>: I am co-organizing a weekly seminar on <a href="https://vsehwag.github.io/SPML_seminar/">Security & Privacy in Machine Learning</a>. If you are interested, please <a href="https://groups.google.com/forum/#!forum/spml-seminars/join">join our mailing list</a> or <a href="https://calendar.google.com/calendar/u/0?cid=N2FwbTVxYzJsOGM2bXBiNGY4am1oMjNsdGNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ"> subscribe to our event calendar</a>.</p> -->
                </td>
            </tr>
        </tbody></table>
        <hr>

        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>Recent News</heading>
                </td>
            </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <ul>
<!--              <li><strong>[<del><font color="red">Call for Papers</font></del>]</strong> I am co-organizing the Workshop on <a href="https://adv-workshop-2020.github.io/">Adversarial Machine Learning in Computer Vision</a> in CVPR 2020. <del>Please submit your paper <a href="https://cmt3.research.microsoft.com/CVPRamlcv2020">here</a>!</del></li>-->
              <!-- <li><strong>[<font color="red">Workshop@ICLR2021</font>]</strong> I am co-organizing the Workshop on <a href="https://aisecure-workshop.github.io/aml-iclr2021/">Security and Safety in Machine Learning Systems</a> in ICLR 2021.</li>
              <li><strong>[<font color="red">Workshop@CVPR2021</font>]</strong> I am co-organizing the Workshop on <a href="https://cvpr21-nas.com/">Neural Architecture Search: 1st Lightweight NAS Challenge and Moving Beyond</a> in CVPR 2021. Stay tuned for more details coming soon.</li>-->
              
              <!-- <li><strong>[<font color="red">February 2023</font>]</strong> I will be giving a talk in <a href="https://practical-dl.github.io/">AAAI 2023 2nd International Workshop on Practical Deep Learning in the Wild</a>.</li> 
               -->
              <li><strong>[May 2024]</strong> Congratulations to <a href="https://xhl-video.github.io/xianhangli/">Xianhang Li</a> on winning the Jack Baskin & Peggy Downes-Baskin Fellowship. Additionally, D-iGPT is accepted by ICML 2024 --- our strongest model secures an ImageNet top-1 accuracy of <strong>90.0%</strong> with ViT-H.</li>

              <li><strong>[April 2024]</strong> We release <a href="https://thefllood.github.io/HQEdit_web/">HQ-Edit</a>, a dataset with high-resolution images & detailed and aligned editing instructions. Our fine-tuned InstructPix2Pix delivers superior editing performance. Additionally, two papers are accepted by TMLR.</li>

              <li><strong>[March 2024]</strong> One paper is accepted by NAACL 2024, and one paper is accepted by TMLR.</li>

              <li><strong>[February 2024]</strong> Three papers (AdvXL, MixCon3D, L2B) are accepted by CVPR 2024.</li>

              <li><strong>[January 2024]</strong> Our <strong>Tuning LayerNorm in Attention</strong> is accepted by ICLR 2024 as a spotlight paper; our visual probing is accpted by EACL 2024.</li>

              <li><strong>[<font color="red">NEW</font>]</strong> Congratulations to <a href="https://asillycat.github.io/">Zijun</a> and the team on winning the <strong>2nd</strong> place in both the base model subtrack and the large model subtrack, Red Teaming Track, in <a href="https://trojandetection.ai/">NeurIPS 2023 Trojan Detection Challenge</a></strong>. We will release the code and the report soon.</li>

              <li><strong>[December 2023]</strong> One paper is accepted by TMLR. Additionally, we release <a href="https://github.com/OliverRensu/D-iGPT">D-iGPT</a>, which attains 89.5% ImageNet top-1 accuracy with ViT-L. Larger models are coming!</li>

              <li><strong>[November 2023]</strong> <a href="https://github.com/UCSC-VLAA/CLIPA">CLIPA-v2</a> is accepted by NeurIPS 2023 R0-FoMo Workshop, and <a href="https://github.com/UCSC-VLAA/Sight-Beyond-Text">Sight Beyond Text</a> is accepted by NeurIPS 2023 Instruction Workshop.</li>

              <li><strong>[October 2023]</strong> Congratulations to <a href="https://laos-y.github.io/">Siwei</a> and the team on winning the <strong>2nd</strong> place in <a href="https://www.synapse.org/#!Synapse:syn51156910/wiki/621282">Task 4: Brain Metastases Segmentation of MICCAI 2023 BraTS Challenge</a>.

              <li><strong>[September 2023]</strong> CLIPA is accepted by NeurIPS 2023. In addition, we release our best model, <a href="https://github.com/UCSC-VLAA/CLIPA">CLIPA-G/14</a>, which attains <strong>83.0%</strong> zero-shot ImageNet top-1 accuracy.</li>

              <li><strong>[<font color="red">NEW</font>]</strong> We release <a href="https://github.com/UCSC-VLAA/CLIPA">CLIPA</a>, which enables CLIP training with limited computational resources. Furthermore, at a low cost of just $15,000, our CLIPA-v2 effectively elevates the zero-shot ImageNet top-1 accuracy to an impressive <strong>81.8%</strong>.</li> 

              <li><strong>[July 2023]</strong> Three papers are accepted by ICCV 2023.</li>

              <li><strong>[June 2023]</strong> Two papers are accepted by MICCAI 2023.</li>

              <li><strong>[February 2023]</strong> DMAE is accepted by CVPR 2023.</li>

              <li><strong>[January 2023]</strong> Two papers are accepted by ICLR 2023.</li>

              <li><strong>[December 2022]</strong> One paper is accepted by IEEE TPAMI.</li>

              <li><strong>[November 2022]</strong> One paper is accepted by AAAI 2023.</li>

              <!-- <li><strong>[October 2022]</strong> I gave a talk in <a href="https://eccv22-arow.github.io/">ECCV 2022 Workshop on Adversarial Robustness in the Real World</a>.</li> -->

              <li><strong>[September 2022]</strong> Two papers are accepted by NeurIPS 2022.</li>

              <li><strong>[July 2022]</strong> Two papers are accepted by ECCV 2022.</li>

              <!-- <li><strong>[June 2022]</strong> I gave a talk in <a href="https://artofrobust.github.io/">CVPR 2022 Workshop on The Art of Robustness: Devil and Angel in Adversarial Machine Learning</a>.</li> -->

              <li><strong>[March 2022]</strong> Two papers are accepted by CVPR 2022.</li>

              <!-- <li><strong>[February 2022]</strong> I gave a talk in <a href="https://practical-dl.github.io/">AAAI 2022 1st International Workshop on Practical Deep Learning in the Wild</a>.</li> -->

              <li><strong>[January 2022]</strong> <a href="https://openreview.net/forum?id=hcoswsDHNAW">Fast AdvProp</a> and <a href="https://openreview.net/forum?id=ydopy-e6Dg">iBOT</a> are accepted by ICLR 2022.</li>

              <!-- <li><strong>[December 2021]</strong> I gave a talk in <a href="http://ai.bu.edu/visda-2021/">NeurIPS 2021 Visual Domain Adaptation Challenge</a>.</li>

              <li><strong>[October 2021]</strong> I gave a talk in <a href="https://iccv21-adv-workshop.github.io/">ICCV 2021 2nd Workshop on Adversarial Robustness In the Real World</a>.</li>

              <li><strong>[September 2021]</strong> Our work on <a href="https://arxiv.org/abs/2111.05464">Robustness Comparison between Transformers and CNNs</a> is accepted by NeurIPS 2021.</li>

              <li><strong>[August 2021]</strong> Our work on <a href="https://arxiv.org/abs/2110.00519">Towards Symbolic Reasoning on Real Images</a> is accepted by ICCV 2021.</li>

              <li><strong>[July 2021]</strong> I gave a talk in <a href="https://advml-workshop.github.io/icml2021/">ICML 2021 Workshop on A Blessing in Disguise: The Prospects and Perils of Adversarial Machine Learning</a>.</li>
              
              <li><strong>[June 2021]</strong> I gave a talk in <a href="https://advmlincv.github.io/cvpr21-tutorial/">CVPR 2021 Tutorial on Adversarial Machine Learning in Computer Vision</a>.</li> -->
             
              <!-- <li><strong>[June 2021]</strong> I am serving as an Area Chair for ICLR 2022.</li> -->
              
              <!-- <li><strong>[February 2021]</strong> Our work on <a href="https://arxiv.org/abs/2103.13886">Robust and Accurate Object Detection</a> is accepted by CVPR 2021.</li> -->
              
              <!-- <li><strong>[January 2021]</strong> Our work on <a href="https://arxiv.org/abs/2010.05981">Shape-Texture Debiased Learning</a> is accepted by ICLR 2021.</li> -->

              <li> I am serving as an Area Chair for CVPR 2024/2023, ICLR 2024/2023/2022, ICML 2024/2023, ICCV 2023/2021, NeurIPS 2023/2022, ECCV 2022, and a Senior Program Committee for AAAI 2022, IJCAI 2021.</li>
              
              
              
              <!-- <li><strong>[October 2020]</strong> I am serving as a Senior Program Committee for IJCAI 2021.</li> -->
              
              <!-- <li><strong>[August 2020]</strong> I am co-organizing the Workshop on <a href="https://eccv20-adv-workshop.github.io/">Adversarial Robustness in the Real World</a> in ECCV 2020 <a href="https://www.youtube.com/watch?v=lDptIbsMIE0&list=PLWqw4ACzC-1XnyywVl53wc7lEOzSyhHYM">[video]</a>.</li> -->
              
              <!-- <li><strong>[July 2020]</strong> Two papers [<a href="https://arxiv.org/abs/1904.00979">1</a>, <a href="https://arxiv.org/abs/2004.05682">2</a>] are accepted by ECCV 2020.</li> -->
              
              <!-- <li><strong>[June 2020]</strong> I am co-organizing the Workshop on <a href="https://adv-workshop-2020.github.io/">Adversarial Machine Learning in Computer Vision</a> in CVPR 2020 <a href="https://www.youtube.com/watch?v=FCTf5MeIBFM&list=PLT6XqV0BKKrKN7GwULt9Z6hVxYd3PSaoy">[video]</a>.</li> -->
              <!-- <li><strong>[February 2020]</strong> Three papers [<a href="https://arxiv.org/abs/1911.09665">1</a>, <a href="https://arxiv.org/abs/2004.01961">2</a>, <a href="https://arxiv.org/abs/1909.04326">3</a>] are accepted by CVPR 2020.</li> -->
              <!-- <li><strong>[December 2019]</strong> <a href="https://openreview.net/forum?id=HyxJhCEFDS">One paper</a> is accepted by ICLR 2020.</li>
              <li><strong>[November 2019]</strong> <a href="https://cs.jhu.edu/~alanlab/Pubs20/li2020learning.pdf">One paper</a> is accepted by AAAI 2020.</li> -->
            </ul>
          </td>
        </tr>
        </tbody></table>
        <hr>


 <!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>Events</heading>
                </td>
            </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <ul> -->
<!--              <li><strong>[<del><font color="red">Call for Papers</font></del>]</strong> I am co-organizing the Workshop on <a href="https://adv-workshop-2020.github.io/">Adversarial Machine Learning in Computer Vision</a> in CVPR 2020. <del>Please submit your paper <a href="https://cmt3.research.microsoft.com/CVPRamlcv2020">here</a>!</del></li>-->

              <!-- <li><strong>[Workshop@ICCV2023]</strong> I am co-organizing the 4th Workshop on <a href="https://iccv23-arow.github.io/">Adversarial Robustness in the Real World</a> in ICCV 2023.</li> -->

              <!-- <li><strong>[<font color="red">Workshop@CVPR2023</font>]</strong> I am co-organizing the 3rd Workshop on <a href="">Adversarial Machine Learning on Computer Vision: Art of Robustness</a> in CVPR 2023.</li>  -->

              <!-- <li><strong>[Workshop@CVPR2022]</strong> I am co-organized the Workshop on <a href="https://artofrobust.github.io/">The Art of Robustness: Devil and Angel in Adversarial Machine Learning</a> in CVPR 2022.</li> -->

              <!-- <li><strong>[Workshop@ICLR2022]</strong> I am co-organized the Workshop on <a href="https://iclrsrml.github.io/">Socially Responsible Machine Learning</a> in ICLR 2022.</li> -->

              <!-- <li><strong>[Journal@Frontiers]</strong> I served as the Guest Editor of <a href="https://www.frontiersin.org/research-topics/17616/trustworthy-machine-learning">Frontiers in Big Data: Trustworthy Machine Learning</a>.</li> -->

              <!-- <li><strong>[Workshop@ICCV2021]</strong> I co-organized the 2nd Workshop on <a href="https://iccv21-adv-workshop.github.io/">Adversarial Robustness in the Real World</a> in ICCV 2021.</li> -->
                <!-- Please submit your paper <a href="https://cmt3.research.microsoft.com/AROW2021/">here</a>.</li> -->

              <!-- <li><strong>[Workshop@ICCV2021]</strong> I co-organized the Workshop on <a href="https://neural-architecture-ppf.github.io/">Neural Architectures: Past, Present and Future</a> in ICCV 2021.</li> -->
               <!-- Please submit your paper <a href="https://cmt3.research.microsoft.com/NeurArch2021">here</a>.</li> -->

			   <!-- <li><strong>[Workshop@ICML2021]</strong> I co-organized the Workshop on <a href="https://icmlsrml2021.github.io/">Socially Responsible Machine Learning</a> in ICML 2021.</a></li> -->

              <!-- <li><strong>[Workshop@CVPR2021]</strong> I co-organized the Workshop on <a href="https://cvpr21-nas.com/">Neural Architecture Search: 1st Lightweight NAS Challenge and Moving Beyond</a> in CVPR 2021.</li> -->
              
              <!-- <li><strong>[Tutorial@CVPR2021]</strong> I co-organized the Tutorial on <a href="https://advmlincv.github.io/cvpr21-tutorial/">Adversarial Machine Learning in Computer Vision</a> in CVPR 2021. The video recording is <a href="https://www.youtube.com/watch?v=AOfq-0Vhprc&list=PLT6XqV0BKKrLEXxUqW7k88pldpdYV1ldF">here</a>.</li> -->

              <!-- <li><strong>[Workshop@ICLR2021]</strong> I co-organized the Workshop on <a href="https://aisecure-workshop.github.io/aml-iclr2021/">Security and Safety in Machine Learning Systems</a> in ICLR 2021.</li> -->
              
              <!-- <li><strong>[Workshop@ECCV2020]</strong> I co-organized the Workshop on <a href="https://eccv20-adv-workshop.github.io/">Adversarial Robustness in the Real World</a> in ECCV 2020. The video recording is <a href="https://www.youtube.com/watch?v=lDptIbsMIE0&list=PLWqw4ACzC-1XnyywVl53wc7lEOzSyhHYM">here</a>.</li> -->
              <!-- <a href="https://www.youtube.com/watch?v=lDptIbsMIE0&list=PLWqw4ACzC-1XnyywVl53wc7lEOzSyhHYM">[video]</a> -->

              <!-- <li><strong>[Workshop@CVPR2020]</strong> I co-organized the Workshop on <a href="https://adv-workshop-2020.github.io/">Adversarial Machine Learning in Computer Vision</a> in CVPR 2020. The video recording is <a href="https://www.youtube.com/watch?v=FCTf5MeIBFM&list=PLT6XqV0BKKrKN7GwULt9Z6hVxYd3PSaoy">here</a>.</li> -->
              <!-- <a href="https://www.youtube.com/watch?v=FCTf5MeIBFM&list=PLT6XqV0BKKrKN7GwULt9Z6hVxYd3PSaoy">[video]</a> -->
 <!--            </ul>
          </td>
        </tr>
        </tbody></table>
        <hr>
 -->

<!--         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>Fundings</heading>
                </td>
            </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <ul>
              <li>I received <a href="https://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/uc-santa-cruz-xie-adversarial-robustness">a $265,000 gift from Open Philanthropy and Good Ventures Foundation</a> for supporting the adversarial machine learning research in my lab.</li>
            </ul>
          </td>
        </tr>
        </tbody></table>
        <hr> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>Current Students</heading>
                </td>
            </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <ul>
			        <li><a href="https://xhl-video.github.io/xianhangli/">Xianhang Li</a> (Ph.D. of 2021, UC Santa Cruz Chancellor's Fellowship, Jack Baskin & Peggy Downes-Baskin Fellowship)</li>
			        <li><a href="https://zw615.github.io/">Zeyu Wang</a> (Ph.D. of 2021)</li>
              <li><a href="https://laos-y.github.io/">Siwei Yang</a> (Ph.D. of 2023)</li>
              <li><a href="https://thefllood.github.io/mudehui.github.io/">Mude Hui</a> (Ph.D. of 2023, co-supervised with Yuyin Zhou)</li>
              <li><a href="https://scholar.google.com/citations?user=8iBtvCQAAAAJ">Jinrui Yang</a> (Ph.D. of 2023, co-supervised with Yuyin Zhou)</li>
              <li><a href="https://asillycat.github.io/">Zijun Wang</a> (Ph.D. of 2024)</li>
              <li><a href="https://scholar.google.com/citations?user=2obvvPoAAAAJ">Yanqing Liu</a> (Ph.D. of 2024)</li>
              <li><a href="https://scholar.google.com/citations?user=hyFMd54AAAAJ">Haoqin Tu</a> (Ph.D. of 2024, UC Santa Cruz Chancellor's Fellowship Recipient, co-supervised with Yuyin Zhou)</li>
              <li><a href="https://xk-huang.github.io/">Xiaoke Huang</a> (Ph.D. of 2024, co-supervised with Yuyin Zhou)</li>
              <li><a href="https://scholar.google.com/citations?user=RSn2gykAAAAJ">Juncheng Wu</a> (Ph.D. of 2024, co-supervised with Yuyin Zhou)</li>
              <li><a href="https://weichen582.github.io/">Chen Wei</a> (affiliated Ph.D. student from Johns Hopkins University)</li>
              <li><a href="https://lambert-x.github.io/">Junfei Xiao</a> (affiliated Ph.D. student from Johns Hopkins University)</li>
              <li><a href="https://oliverrensu.github.io/">Sucheng Ren</a> (affiliated Ph.D. student from Johns Hopkins University)</li>
              <li><a href="https://scholar.google.com/citations?user=aqwyG_YAAAAJ">Lei Zhang</a> (affiliated Ph.D. student from UC San Diego)</li>
              <li><a href="https://bzhao.me/">Bingchen Zhao</a> (visiting graduate student from University of Edinburgh)</li>
              <li><a href="https://scholar.google.com/citations?user=8Fq3EFkAAAAJ">Fangxun Shu</a> (visiting graduate student in Summer'24)</li>
              <li><a href="https://github.com/yunfeixie233">Yunfei Xie</a> (visiting undergraduate student in Summer'24)</li>
            </ul>
          </td>
        </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>Alumni</heading>
                </td>
            </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <h5><u>2023</u></h5>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <ul>
              <li><a href="https://asillycat.github.io/">Zijun Wang</a> (visiting undergraduate student in Summer'23; next: Ph.D. at UC Santa Cruz)</li>
              <li><a href="https://scholar.google.com/citations?user=hyFMd54AAAAJ&hl">Haoqin Tu</a> (visiting graduate student in Summer'23; next: Ph.D. at UC Santa Cruz)</li>
              <li><a href="https://scholar.google.com/citations?user=aqwyG_YAAAAJ">Lei Zhang</a> (visiting graduate student in Summer'23; next: Ph.D. at UC San Diego)</li>
              <li><a href="https://scholar.google.com/citations?user=m8I1ELMAAAAJ">Yipeng Gao</a> (visiting graduate student in Summer'23; next: Ph.D. at University of Southern California)</li>
              <li><a href="https://scholar.google.com/citations?user=nHKExN0AAAAJ">Jieru Mei</a> (affiliated Ph.D. student from Johns Hopkins University; next: Google)</li>
              <li><a href="https://lizw14.github.io/">Zhuowan Li</a> (affiliated Ph.D. student from Johns Hopkins University; next: Google)</li>
              <li><a href="https://scholar.google.com/citations?user=lU3wroMAAAAJ">Yixiao Zhang</a> (affiliated Ph.D. student from Johns Hopkins University; next: Amazon)</li>
            </ul>
          </td>
        </tr>
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <h5><u>2022</u></h5>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <ul>
              <li><a href="https://scholar.google.com.hk/citations?user=X-JmE9UAAAAJ">Sizhe Chen</a> (visiting graduate student in 2022; next: Ph.D. at UC Berkeley)</li>
              <li><a href="https://scholar.google.com/citations?user=0WFC2w0AAAAJ">Yuanze Lin</a> (visiting graduate student in Summer'22; next: Ph.D. at University of Oxford)</li>
              <li><a href="https://scholar.google.com/citations?user=5cY3Ho4AAAAJ">Chen Wang</a> (visiting graduate student in Summer'22; next: Ph.D. at University of Pennsylvania)</li>
              <li><a href="https://huanglizi.github.io/">Zihan Li</a> (visiting graduate student in Summer'22; next: Ph.D. at University of Washington)</li>
              <li><a href="https://jywu511.github.io/">Junyang Wu</a> (visiting undergraduate student in Summer'22; next: Ph.D. at Shanghai Jiao Tong University)</li>
              <li><a href="https://scholar.google.com/citations?user=s1m55YoAAAAJ">Shaoyuan Xie</a> (visiting undergraduate student in Summer'22; next: Ph.D. at UC Irvine)</li>
              <li><a href="https://yqwang01.github.io/">Yiqing Wang</a> (visiting undergraduate student in Summer'22; next: Ph.D. at Duke University)</li>
              <li>Peiran Xu (visiting undergraduate student in Summer'22; next: Ph.D. at UCLA)</li>
              <li>Yunchao Zhang (visiting undergraduate student in Summer'22; next: Ph.D. at The University of Hong Kong)</li>
              <li>Zihao Wei (visiting undergraduate student in Summer'22; next: Master at UMich)</li>
            </ul>
          </td>
        </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <h5><u>2021</u></h5>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <ul>
              <li><a href="https://oliverrensu.github.io/">Sucheng Ren</a> (visiting graduate student in Summer'21; next: Ph.D. at Johns Hopkins University)</li>
              <li><a href="https://shallowtoil.github.io/">Jinghao Zhou</a> (visiting undergraduate student in Summer'21; next: Ph.D. at University of Oxford)</li>
            </ul>
          </td>
        </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <h5><u>2020</u></h5>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <ul>
              <li><a href="https://yingwei.li/">Yingwei Li</a> (affiliated Ph.D. student from Johns Hopkins University; next: Waymo)</li>
              <li><a href="https://tingxueronghua.github.io/">Yucheng Han</a> (visiting undergraduate student in Summer'20; next: Ph.D. at Nanyang Technological University)</li>
            </ul>
          </td>
        </tr>
        </tbody></table>

        <hr>


        
        
        
<!--         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>Recent Talks</heading>
                </td>
            </tr>
        </tbody></table> -->
        
 <!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <ul>
                    <li>Dec 2019: <b>Towards Robust Defense Against Adversarial Examples &amp Beyond</b>, University of Maryland, College Park</li>
                    <li>Dec 2019: <b>Adversarial Examples Improve Image Recognition</b>, Google Brain</li>
                    <li>Sep 2019: <b>Feature Denoising for Improving Adversarial Robustness</b>, VALSE Webinar</li>
                    <li>Summer 2019: <b>Intriguing Adversarial Examples &amp How To Defend Against Them</b>, UC Berkeley, UC San Diego, UC Davis, UC Merced, Stanford University, Gooole Brain</li>
                    <li>May 2019: <b>Towards Transferable Adversarial Attacks & Robust Adversarial Defense</b>, Princeton University</li>
                </ul>
            </tr>
        <hr> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <h4><u>2024</u></h4>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 


         <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/ren2024digpt.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2312.02147">
                    <papertitle>Rejuvenating image-GPT as Strong Visual Representation Learners</papertitle>
                </a>
                <br>
                <a href="https://oliverrensu.github.io/">Sucheng Ren</a>,
                <a href="https://zw615.github.io/">Zeyu Wang</a>,
                <a href="https://scholar.google.com/citations?user=G8NZJLIAAAAJ">Hongru Zhu</a>,
                <a href="https://scholar.google.com/citations?user=rv-aTqkAAAAJ">Junfei Xiao</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                <strong>Cihang Xie</strong>
                <br>
                <em>ICML</em>, 2024
                <br>
                </p>
                <div class="paper" id="ren2024digpt">
                    <a href="https://arxiv.org/pdf/2312.02147.pdf">pdf</a> /
                    <a href="https://github.com/OliverRensu/D-iGPT">project page</a> /
                    <a href="data/ren2024digpt.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--ren2024digpt-->



        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/hui2024hqedit.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2404.09990">
                    <papertitle>HQ-Edit: A High-Quality Dataset for Instruction-based Image Editing</papertitle>
                </a>
                <br>
                <a href="https://thefllood.github.io/mudehui.github.io/">Mude Hui</a>,
                <a href="https://laos-y.github.io/">Siwei Yang</a>,
                <a href="https://bzhao.me/">Bingchen Zhao</a>,              
                <a href="https://seasonsh.github.io/">Yichun Shi</a>, 
                <a href="https://hengcv.github.io/">Heng Wang</a>, 
                <a href="https://pengwangucla.github.io/peng-wang.github.io/">Peng Wang</a>, 
                <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>,
                <strong>Cihang Xie</strong>
                <br>
                <em>arxiv</em>, 2024
                <br>
                </p>
                <div class="paper" id="hui2024hqedit">
                    <a href="https://arxiv.org/pdf/2404.09990.pdf">pdf</a> /
                    <a href="https://thefllood.github.io/HQEdit_web">project page</a> /
                    <a href="data/hui2024hqedit.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--hui2024hqedit-->


        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/yang2024aqa.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2402.09404">
                    <papertitle>AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability</papertitle>
                </a>
                <br>
                <a href="https://laos-y.github.io/">Siwei Yang</a>,
                <a href="https://bzhao.me/">Bingchen Zhao</a>,              
                <strong>Cihang Xie</strong>
                <br>
                <em>arxiv</em>, 2024
                <br>
                </p>
                <div class="paper" id="yang2024aqa">
                    <a href="https://arxiv.org/pdf/2402.09404.pdf">pdf</a> /
                    <a href="https://github.com/UCSC-VLAA/AQA-Bench">project page</a> /
                    <a href="data/yang2024aqa.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--yang2024aqa-->



        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/wang2024advxl.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2401.04727">
                    <papertitle>Revisiting Adversarial Training at Scale</papertitle>
                </a>
                <br>
                <a href="https://zw615.github.io/">Zeyu Wang</a>,
                <a href="https://xhl-video.github.io/xianhangli/">Xianhang Li</a>,
                <a href="https://scholar.google.com/citations?user=G8NZJLIAAAAJ">Hongru Zhu</a>,
                <strong>Cihang Xie</strong>
                <br>
                <em>CVPR</em>, 2024
                <br>
                </p>
                <div class="paper" id="wang2024advxl">
                    <a href="https://arxiv.org/pdf/2401.04727.pdf">pdf</a> /
                    <a href="https://github.com/UCSC-VLAA/AdvXL">project page</a> /
                    <a href="data/wang2024advxl.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--wang2024advxl-->


        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/gao2024mixcon3d.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2311.01734">
                    <papertitle>Sculpting Holistic 3D Representation in Contrastive Language-Image-3D Pre-training</papertitle>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=m8I1ELMAAAAJ">Yipeng Gao</a>,
                <a href="https://zw615.github.io/">Zeyu Wang</a>,
                <a href="https://scholar.google.com/citations?user=AwqDDGoAAAAJ&hl=en">Wei-Shi Zheng</a>,
                <strong>Cihang Xie</strong>,
                <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>
                <br>
                <em>CVPR</em>, 2024
                <br>
                </p>
                <div class="paper" id="gao2024mixcon3d">
                    <a href="https://arxiv.org/pdf/2311.01734.pdf">pdf</a> /
                    <a href="https://github.com/UCSC-VLAA/MixCon3D">project page</a> /
                    <a href="data/gao2024mixcon3d.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--gao2024mixcon3d-->



        <tr bgcolor="#ffffd0">
          <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/zhou2024L2B.png' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2202.04291">
                    <papertitle>Learning to Bootstrap for Combating Label Noise</papertitle>
                </a>
                <br>

                <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>,
                <a href="https://xhl-video.github.io/xianhangli/">Xianhang Li</a>,
                <a href="https://scholar.google.com/citations?user=T3EjsaAAAAAJ&hl=en">Fengze Liu</a>,
                <a href="https://xxchen.site/">Xuxi Chen</a>,
                <a href="https://yulequan.github.io/">Lequan Yu</a>,
                <strong>Cihang Xie</strong>,
                <a href="https://scholar.google.com/citations?user=z1UtMSYAAAAJ&hl=en">Matthew P. Lungren</a>,
                <a href="https://profiles.stanford.edu/lei-xing">Lei Xing</a>
                <br>
                <em>CVPR</em>, 2024
                <br>
                </p>
                <div class="paper" id="zhou2024L2B">
                    <a href="https://arxiv.org/pdf/2202.04291.pdf">pdf</a> /
                    <a href="https://github.com/yuyinzhou/L2B">project page</a> /
                    <a href="data/zhou2024L2B.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--zhou2024L2B-->


        <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/wang2024benchmarking.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2301.04075">
                    <papertitle>Benchmarking Robustness in Neural Radiance Fields</papertitle>
                </a>
                <br>
                <a href="https://cwchenwang.github.io/">Chen Wang</a>,
                <a href="https://scholar.google.ch/citations?user=YR7re-cAAAAJ">Angtian Wang</a>,
                <a href="https://ljb121002.github.io/">Junbo Li</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                <strong>Cihang Xie</strong>
                <br>
                <em>CVPR Adversarial Machine Learning Workshop</em>, 2024
                <br>
                </p>
                <div class="paper" id="wang2024benchmarking">
                    <a href="https://arxiv.org/pdf/2301.04075.pdf">pdf</a> /
                    <a href="data/wang2024benchmarking.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--wang2024benchmarking-->



        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/zhao2024tuning.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2312.11420">
                    <papertitle>Tuning LayerNorm in Attention: Towards Efficient MultiModal LLM Finetuning</papertitle>
                </a>
                <br>
                <a href="https://bzhao.me/">Bingchen Zhao</a>,
                <a href="https://scholar.google.com/citations?user=hyFMd54AAAAJ&hl">Haoqin Tu</a>,
                <a href="https://weichen582.github.io/">Chen Wei</a>,
                <a href="https://meijieru.com/">Jieru Mei</a>,
                <strong>Cihang Xie</strong>
                <br>
                <em>ICLR</em>, 2024
                <br>
                </p>
                <div class="paper" id="zhao2024tuning">
                    <a href="https://arxiv.org/pdf/2312.11420.pdf">pdf</a> /
                    <a href="https://huggingface.co/docs/peft/main/en/package_reference/layernorm_tuning">project page</a> /
                    <a href="data/zhao2024tuning.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--zhao2024tuning-->



        <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/zhang2024navigation.jpg' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2211.14769">
                    <papertitle>Navigation as the Attacker Wishes? Towards Building Byzantine-Robust Embodied Agents under Federated Learning</papertitle>
                </a>
                <br>
                <a href="">Yunchao Zhang</a>,
                <a href="https://elegantlin.github.io/">Zonglin Di</a>,
                <a href="https://kevinz-01.github.io/">Kaiwen Zhou</a>,
                <strong>Cihang Xie</strong>,
                <a href="https://eric-xw.github.io/">Xin Eric Wang</a>
                <br>
                <em>NAACL</em>, 2024
                <br>
                </p>
                <div class="paper" id="zhang2024navigation">
                    <a href="https://arxiv.org/pdf/2211.14769.pdf">pdf</a> /
                    <a href="https://github.com/eric-ai-lab/Naivgation-as-wish">project page</a> /
                    <a href="data/zhang2024navigation.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--zhang2024navigation-->



        <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/li2024probing.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2211.11446">
                    <papertitle>Localization vs. Semantics: Visual Representations in Unimodal and Multimodal Models</papertitle>
                </a>
                <br>
                <a href="https://lizw14.github.io/">Zhuowan Li</a>,
                <strong>Cihang Xie</strong>,
                <a href="https://www.cs.jhu.edu/~vandurme/">Benjamin Van Durme</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                <br>
                <em>EACL</em>, 2024
                <br>
                </p>
                <div class="paper" id="li2024probing">
                    <a href="https://arxiv.org/pdf/2212.00281.pdf">pdf</a> /
                    <a href="https://github.com/Lizw14/visual_probing">project page</a> /
                    <a href="data/li2024probing.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--li2024probing-->


        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/xie2024adversarial.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2301.10766">
                    <papertitle>On the Adversarial Robustness of Camera-based 3D Object Detection</papertitle>
                </a>
                <br>
                <a href="">Shaoyuan Xie</a>,
                <a href="https://zichaoli.github.io/">Zichao Li</a>,
                <a href="https://zw615.github.io/">Zeyu Wang</a>,
                <strong>Cihang Xie</strong>
                <br>
                <em>TMLR</em>, 2024
                <br>
                </p>
                <div class="paper" id="xie2024adversarial">
                    <a href="https://arxiv.org/pdf/2301.10766.pdf">pdf</a> /
                    <a href="https://github.com/Daniel-xsy/BEV-Attack">project page</a> /
                    <a href="data/xie2024adversarial.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--xie2024adversarial-->


        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/wu2024evp.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2212.10556">
                    <papertitle>Unleashing the Power of Visual Prompting At the Pixel Level</papertitle>
                </a>
                <br>
                <a href="https://jywu511.github.io/">Junyang Wu</a>,
                <a href="https://xhl-video.github.io/xianhangli/">Xianhang Li</a>,
                <a href="https://weichen582.github.io/">Chen Wei</a>,
                <a href="https://csrhddlam.github.io/">Huiyu Wang</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>,
                <strong>Cihang Xie</strong>
                <br>
                <em>TMLR</em>, 2024
                <br>
                </p>
                <div class="paper" id="wu2024evp">
                    <a href="https://arxiv.org/pdf/2212.10556.pdf">pdf</a> /
                    <a href="https://github.com/UCSC-VLAA/EVP">project page</a> /
                    <a href="data/wu2024evp.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--wu2024evp-->


        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/xu2024fedconv.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2310.04412">
                    <papertitle>FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning</papertitle>
                </a>
                <br>
                <a href="">Peiran Xu</a>,
                <a href="https://zw615.github.io/">Zeyu Wang</a>,
                <a href="https://meijieru.com/">Jieru Mei</a>,
                <a href="https://scholar.google.com/citations?user=ruKpgzwAAAAJ">Liangqiong Qu</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                <strong>Cihang Xie</strong>,
                <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>
                <br>
                <em>TMLR</em>, 2024
                <br>
                </p>
                <div class="paper" id="xu2024fedconv">
                    <a href="https://arxiv.org/pdf/2310.04412.pdf">pdf</a> /
                    <a href="https://github.com/UCSC-VLAA/FedConv">project page</a> /
                    <a href="data/xu2024fedconv.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--xu2024fedconv-->


        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/li2024scaling.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://openreview.net/forum?id=t4nnCi5AO6">
                    <papertitle>Scaling (Down) CLIP: A Comprehensive Analysis of Data, Architecture, and Training Strategies</papertitle>
                </a>
                <br>
                <a href="https://zichaoli.github.io/">Zichao Li</a>,
                <strong>Cihang Xie</strong>,
                <a href="https://scholar.google.com/citations?user=Mu_8iOEAAAAJ">Ekin Dogus Cubuk</a>
                <br>
                <em>TMLR</em>, 2024
                <br>
                </p>
                <div class="paper" id="li2024scaling">
                    <a href="https://openreview.net/forum?id=t4nnCi5AO6">pdf</a> /
                    <a href="data/li2024scaling.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--li2024scaling-->


        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/mei2024spformer.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2401.02931">
                    <papertitle>SPFormer: Enhancing Vision Transformer with Superpixel Representation</papertitle>
                </a>
                <br>
                <a href="https://meijieru.com/">Jieru Mei</a>,
                <a href="http://liangchiehchen.com/">Liang-Chieh Chen</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,                
                <strong>Cihang Xie</strong>
                <br>
                <em>arxiv</em>, 2024
                <br>
                </p>
                <div class="paper" id="mei2024spformer">
                    <a href="https://arxiv.org/pdf/2401.02931.pdf">pdf</a> /
                    <a href="data/mei2024spformer.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--mei2024spformer-->



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <h4><u>2023</u></h4>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 



       


        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/xiao2023semantic.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2312.13764">
                    <papertitle>A Semantic Space is Worth 256 Language Descriptions: Make Stronger Segmentation Models with Descriptive Properties</papertitle>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=rv-aTqkAAAAJ">Junfei Xiao</a>,
                <a href="https://zzzqzhou.github.io/">Ziqi Zhou</a>,
                <a href="https://scholar.google.com/citations?user=tpNZM2YAAAAJ">Wenxuan Li</a>,
                <a href="https://voidrank.github.io/">Shiyi Lan</a>,
                <a href="https://meijieru.com/">Jieru Mei</a>,
                <a href="https://scholar.google.com/citations?user=1VI_oYUAAAAJ">Zhiding Yu</a>,
                <a href="https://bzhao.me/">Bingchen Zhao</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>,
                <strong>Cihang Xie</strong>
                <br>
                <em>arxiv</em>, 2023
                <br>
                </p>
                <div class="paper" id="xiao2023semantic">
                    <a href="https://arxiv.org/pdf/2312.13764.pdf">pdf</a> /
                    <a href="https://github.com/lambert-x/ProLab">project page</a> /
                    <a href="data/xiao2023semantic.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--xiao2023semantic-->



        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/shu2023audio.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2312.06720">
                    <papertitle>Audio-Visual LLM for Video Understanding</papertitle>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=8Fq3EFkAAAAJ">Fangxun Shu</a>,
                <a href="https://scholar.google.com/citations?user=aqwyG_YAAAAJ">Lei Zhang</a>,
                <a href="https://scholar.google.com/citations?user=0TvdOEcAAAAJ">Hao Jiang</a>,
                <strong>Cihang Xie</strong>
                <br>
                <em>arxiv</em>, 2023
                <br>
                </p>
                <div class="paper" id="shu2023audio">
                    <a href="https://arxiv.org/pdf/2312.06720.pdf">pdf</a> /
                    <a href="data/shu2023audio.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--shu2023audio-->


        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/zhang2023compress.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2312.06726">
                    <papertitle>Compress & Align: Curating Image-Text Data with Human Knowledge</papertitle>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=aqwyG_YAAAAJ">Lei Zhang</a>,
                <a href="">Fangxun Shu</a>,
                <a href="https://oliverrensu.github.io/">Sucheng Ren</a>,
                <a href="">Hao Jiang</a>,
                <a href="https://bzhao.me/">Bingchen Zhao</a>,
                <strong>Cihang Xie</strong>
                <br>
                <em>arxiv</em>, 2023
                <br>
                </p>
                <div class="paper" id="zhang2023compress">
                    <a href="https://arxiv.org/pdf/2312.06726.pdf">pdf</a> /
                    <a href="data/zhang2023compress.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--zhang2023compress-->


        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/tu2023unicorns.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2311.16101">
                    <papertitle>How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs</papertitle>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=hyFMd54AAAAJ&hl">Haoqin Tu</a>,
                <a href="https://scholar.google.com/citations?user=V5X1gdAAAAAJ">Chenhang Cui</a>,
                <a href="https://asillycat.github.io/">Zijun Wang</a>,
                <a href="https://scholar.google.com/citations?user=6KltFMAAAAAJ">Yiyang Zhou</a>,
                <a href="https://bzhao.me/">Bingchen Zhao</a>,
                <a href="https://junlinhan.github.io/">Junlin Han</a>,
                <a href="https://michaelzhouwang.github.io/">Wangchunshu Zhou</a>,
                <a href="https://www.huaxiuyao.io/">Huaxiu Yao</a>,
                <strong>Cihang Xie</strong>
                <br>
                <em>arxiv</em>, 2023
                <br>
                </p>
                <div class="paper" id="tu2023unicorns">
                    <a href="https://arxiv.org/pdf/2311.16101.pdf">pdf</a> /
                    <a href="https://github.com/UCSC-VLAA/vllm-safety-benchmark">project page</a> /
                    <a href="data/tu2023unicorns.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--tu2023unicorns-->



        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/tu2023sight.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2309.07120">
                    <papertitle>Sight Beyond Text: Multi-Modal Training Enhances LLMs in Truthfulness and Ethics</papertitle>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=hyFMd54AAAAJ&hl">Haoqin Tu</a>,
                <a href="https://bzhao.me/">Bingchen Zhao</a>,
                <a href="https://weichen582.github.io/">Chen Wei</a>,
                <strong>Cihang Xie</strong>
                <br>
                <em>NeurIPS Instruction Workshop</em>, 2023
                <br>
                </p>
                <div class="paper" id="tu2023sight">
                    <a href="https://arxiv.org/pdf/2309.07120.pdf">pdf</a> /
                    <a href="https://github.com/UCSC-VLAA/Sight-Beyond-Text">project page</a> /
                    <a href="data/tu2023sight.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--li2023clipa-->


        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/li2023clipav2.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2306.15658">
                    <papertitle>CLIPA-v2: Scaling CLIP Training with 81.1% Zero-shot ImageNet Accuracy within a $10,000 Budget; An Extra $4,000 Unlocks 81.8% Accuracy</papertitle>
                </a>
                <br>
                <a href="https://xhl-video.github.io/xianhangli/">Xianhang Li</a>,
                <a href="https://zw615.github.io/">Zeyu Wang</a>,
                <strong>Cihang Xie</strong>
                <br>
                <em>NeurIPS R0-FoMo Workshop</em>, 2023
                <br>
                </p>
                <div class="paper" id="li2023clipav2">
                    <a href="https://arxiv.org/pdf/2306.15658.pdf">pdf</a> /
                    <a href="https://github.com/UCSC-VLAA/CLIPA">project page</a> /
                    <a href="data/li2023clipav2.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--li2023clipa-->


        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/li2023clipa.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2305.07017">
                    <papertitle>An Inverse Scaling Law for CLIP Training</papertitle>
                </a>
                <br>
                <a href="https://xhl-video.github.io/xianhangli/">Xianhang Li</a>,
                <a href="https://zw615.github.io/">Zeyu Wang</a>,
                <strong>Cihang Xie</strong>
                <br>
                <em>NeurIPS</em>, 2023
                <br>
                </p>
                <div class="paper" id="li2023clipa">
                    <a href="https://arxiv.org/pdf/2305.07017.pdf">pdf</a> /
                    <a href="https://github.com/UCSC-VLAA/CLIPA">project page</a> /
                    <a href="data/li2023clipa.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--li2023clipa-->


        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/wei2023DiffMAE.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2304.03283">
                    <papertitle>Diffusion Models as Masked Autoencoders</papertitle>
                </a>
                <br>
                <a href="https://weichen582.github.io/">Chen Wei</a>,
                <a href="https://karttikeya.github.io/">Karttikeya Mangalam</a>,
                <a href="http://www.cs.cmu.edu/~poyaoh/">Po-Yao Huang</a>,
                <a href="https://lyttonhao.github.io/">Yanghao Li</a>,
                <a href="https://haoqifan.github.io/">Haoqi Fan</a>,
                <a href="https://howardhsu.github.io/">Hu Xu</a>,
                <a href="https://csrhddlam.github.io/">Huiyu Wang</a>,
                <strong>Cihang Xie</strong>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                <a href="https://feichtenhofer.github.io/">Christoph Feichtenhofer</a>
                <br>
                <em>ICCV</em>, 2023
                <br>
                </p>
                <div class="paper" id="wei2023DiffMAE">
                    <a href="https://arxiv.org/pdf/2304.03283.pdf">pdf</a> /
                    <a href="https://weichen582.github.io/diffmae.html">project page</a> /
                    <a href="data/wei2023DiffMAE.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--wei2023DiffMAE-->


        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/lin2023smaug.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2211.11446">
                    <papertitle>SMAUG: Sparse Masked Autoencoder for Efficient Video-Language Pre-training</papertitle>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=0WFC2w0AAAAJ">Yuanze Lin</a>,
                <a href="https://weichen582.github.io/">Chen Wei</a>,
                <a href="https://csrhddlam.github.io/">Huiyu Wang</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                <strong>Cihang Xie</strong>
                <br>
                <em>ICCV</em>, 2023
                <br>
                </p>
                <div class="paper" id="lin2023smaug">
                    <a href="https://arxiv.org/pdf/2211.11446.pdf">pdf</a> /
                    <a href="data/lin2023smaug.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--lin2023smaug-->


        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/wang2023DistillBEV.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2309.15109">
                    <papertitle>DistillBEV: Boosting Multi-Camera 3D Object Detection with Cross-Modal Knowledge Distillation</papertitle>
                </a>
                <br>
                <a href="https://zw615.github.io/">Zeyu Wang</a>,
                <a href="https://scholar.google.com/citations?user=JAkiArcAAAAJ">Dingwen Li</a>,
                <a href="https://scholar.google.com/citations?user=yevdegIAAAAJ">Chenxu Luo</a>,
                <strong>Cihang Xie</strong>,
                <a href="https://scholar.google.com/citations?user=yWsMg_gAAAAJ">Xiaodong Yang</a>
                <br>
                <em>ICCV</em>, 2023
                <br>
                </p>
                <div class="paper" id="wang2023BEVDistill">
                    <a href="https://arxiv.org/pdf/2309.15109.pdf">pdf</a> /
                    <a href="data/wang2023DistillBEV.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--wang2023BEVDistill-->


        <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/wang2023SwinMM.png' width="250"></div>
            </td>
            <td width="75%" valign="middle">
                <p>
                    <a href="https://arxiv.org/abs/2307.12591">
                        <papertitle>SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation</papertitle>
                    </a>
                    <br>
                    <a href="https://yqwang01.github.io/">Yiqing Wang</a>,
                    <a href="https://huanglizi.github.io/">Zihan Li</a>,
                    <a href="https://meijieru.com/">Jieru Mei</a>,
                    <a href="">Zihao Wei</a>,
                    <a href="https://leolee7.github.io/">Li Liu</a>,
                    <a href="https://cwchenwang.github.io/">Chen Wang</a>,
                    <a href="https://github.com/ShengtianSang">Shengtian Sang</a>,
                    <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                    <strong>Cihang Xie</strong>,
                    <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>
                    <br>
                    <em>MICCAI</em>, 2023
                    <br>
                </p>
                <div class="paper" id="wang2023SwinMM">
                    <a href="https://arxiv.org/pdf/2307.12591.pdf">pdf</a> /
                    <a href="https://github.com/UCSC-VLAA/SwinMM/">project page</a> /                    
                    <a href="data/wang2023SwinMM.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr>


        <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/wei2023MLBSeg.png' width="250"></div>
            </td>
            <td width="75%" valign="middle">
                <p>
                    <a href="https://arxiv.org/abs/2307.11604">
                        <papertitle>Consistency-guided Meta-Learning for Bootstrapping Semi-Supervised Medical Image Segmentation</papertitle>
                    </a>
                    <br>
                    <a href="https://profiles.stanford.edu/qingyue-wei">Qingyue Wei</a>,
                    <a href="https://yulequan.github.io/">Lequan Yu</a>,
                    <a href="https://xhl-video.github.io/xianhangli/">Xianhang Li</a>,
                    <a href="https://scholar.google.com/citations?user=m6Q6u2MAAAAJ">Wei Shao</a>,
                    <strong>Cihang Xie</strong>,
                    <a href="https://profiles.stanford.edu/lei-xing">Lei Xing</a>,
                    <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>
                    <br>
                    <em>MICCAI</em>, 2023
                    <br>
                </p>
                <div class="paper" id="wei2023MLBSeg">
                    <a href="https://arxiv.org/pdf/2307.11604.pdf">pdf</a> /
                    <a href="https://github.com/aijinrjinr/MLB-Seg">project page</a> /   
                    <a href="data/wei2023MLBSeg.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr>


        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/bai2022dmae.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2208.12256">
                    <papertitle>Masked Autoencoders Enable Efficient Knowledge Distillers</papertitle>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=N1-l4GsAAAAJ">Yutong Bai</a>,
                <a href="https://zw615.github.io/">Zeyu Wang</a>,
                <a href="https://scholar.google.com/citations?user=rv-aTqkAAAAJ">Junfei Xiao</a>,
                <a href="https://weichen582.github.io/">Chen Wei</a>,
                <a href="https://csrhddlam.github.io/">Huiyu Wang</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>,
                <strong>Cihang Xie</strong>
                <br>
                <em>CVPR</em>, 2023
                <br>
                </p>
                <div class="paper" id="bai2022dmae">
                    <a href="https://arxiv.org/pdf/2208.12256.pdf">pdf</a> /
                    <a href="https://github.com/UCSC-VLAA/DMAE">project page</a> /
                    <a href="data/bai2022dmae.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--bai2022dmae-->


        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/wang2022robustcnn.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2206.03452">
                    <papertitle>Can CNNs Be More Robust Than Transformers?</papertitle>
                </a>
                <br>
                <a href="https://zw615.github.io/">Zeyu Wang</a>,
                <a href="https://scholar.google.com/citations?user=N1-l4GsAAAAJ">Yutong Bai</a>,
                <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>,
                <strong>Cihang Xie</strong>
                <br>
                <em>ICLR</em>, 2023
                <br>
                </p>
                <div class="paper" id="wang2022robustcnn">
                    <a href="https://arxiv.org/pdf/2206.03452.pdf">pdf</a> /
                    <a href="https://github.com/UCSC-VLAA/RobustCNN">project page</a> /
                    <a href="data/wang2022robustcnn.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--wang2022robustcnn-->


        <tr>
          <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/wu2022onepixel.png' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2205.12141">
                    <papertitle>One-Pixel Shortcut: on the Learning Preference of Deep Neural Networks</papertitle>
                </a>
                <br>

                <a href="http://www.pami.sjtu.edu.cn/En/Shutong%20Wu">Shutong Wu</a>,
                <a href="https://scholar.google.com.hk/citations?user=X-JmE9UAAAAJ">Sizhe Chen</a>,
                <strong>Cihang Xie</strong>,
                <a href="https://scholar.google.com/citations?user=DR-gBcEAAAAJ">Xiaolin Huang</a>
                <br>
                <em>ICLR</em>, 2023
                <br>
                </p>
                <div class="paper" id="wu2022onepixel">
                    <a href="https://arxiv.org/pdf/2205.12141.pdf">pdf</a> /
                    <a href="data/wu2022onepixel.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--wu2022onepixel-->


        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/xu2023bnet.jpg' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2011.14150">
                    <papertitle>BNET: Batch Normalization with Enhanced Linear Transformation</papertitle>
                </a>
                <br>

                <a href="https://yuhuixu1993.github.io/">Yuhui Xu</a>,
                <a href="http://lingxixie.com/Home.html">Lingxi Xie</a>,
                <strong>Cihang Xie</strong>,
                <a href="https://en.zhiyuan.sjtu.edu.cn/en/faculty/585/detail">Wenrui Dai</a>,
                <a href="https://meijieru.com/">Jieru Mei</a>,
                <a href="https://www.cs.jhu.edu/~syqiao/">Siyuan Qiao</a>,
                <a href="http://wei-shen.weebly.com/">Wei Shen</a>,
                <a href="http://min.sjtu.edu.cn/xhk.htm">Hongkai Xiong</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>                
                <br>
                <em>IEEE TPAMI</em>, 2023
                <br>
                </p>
                <div class="paper" id="xu2023bnet">
                    <a href="https://arxiv.org/pdf/2011.14150.pdf">pdf</a> /
                    <a href="https://github.com/yuhuixu1993/BNET">project page</a> /
                    <a href="data/xu2023bnet.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--xu2023bnet-->


        <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/nataniel2023practical.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://ojs.aaai.org/index.php/AAAI/article/view/26693">
                    <papertitle>Practical Disruption of Image Translation Deepfake Networks</papertitle>
                </a>
                <br>
                <a href="https://natanielruiz.github.io/">Nataniel Ruiz</a>,
                <a href="https://cs-people.bu.edu/sbargal/">Sarah Adel Bargal</a>,
                <strong>Cihang Xie</strong>,
                <a href="https://www.cs.bu.edu/fac/sclaroff/">Stan Sclaroff</a>
                <br>
                <em>AAAI</em>, 2023
                <br>
                </p>
                <div class="paper" id="nataniel2023practical">
                    <a href="https://ojs.aaai.org/index.php/AAAI/article/view/26693/26465">pdf</a> /
                    <a href="data/nataniel2023practical.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--nataniel2023practical-->


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <h4><u>2022</u></h4>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 



        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/li2022bag.jpg' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2209.02684">
                    <papertitle>Bag of Tricks for FGSM Adversarial Training</papertitle>
                </a>
                <br>
                <a href="https://zichaoli.github.io/">Zichao Li</a>,
                <a href="https://leolee7.github.io/">Li Liu</a>,
                <a href="https://zw615.github.io/">Zeyu Wang</a>,
                <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>,
                <strong>Cihang Xie</strong>
                <br>
                <em>arxiv</em>, 2022
                <br>
                </p>
                <div class="paper" id="li2022bag">
                    <a href="https://arxiv.org/pdf/2209.02684.pdf">pdf</a> /
                    <a href="https://github.com/UCSC-VLAA/Bag-of-Tricks-for-FGSM-AT">project page</a> /
                    <a href="data/li2022bag.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--li2022bag-->


        <tr>
          <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/nataniel2022counterfactualtest.png' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2211.16499">
                    <papertitle>Finding Differences Between Transformers and ConvNets Using Counterfactual Simulation Testing</papertitle>
                </a>
                <br>

                <a href="https://natanielruiz.github.io/">Nataniel Ruiz</a>,
                <a href="https://cs-people.bu.edu/sbargal/">Sarah Adel Bargal</a>,
                <strong>Cihang Xie</strong>,
                <a href="http://ai.bu.edu/ksaenko.html">Kate Saenko</a>,
                <a href="https://www.cs.bu.edu/fac/sclaroff/">Stan Sclaroff</a>
                <br>
                <em>NeurIPS</em>, 2022
                <br>
                </p>
                <div class="paper" id="nataniel2022counterfactualtest">
                    <a href="https://arxiv.org/pdf/2211.16499.pdf">pdf</a> /
                    <a href="https://counterfactualsimulation.github.io.git">project page</a> /
                    <a href="data/nataniel2022counterfactualtest.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--nataniel2022counterfactualtest-->


        <tr>
          <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/chen2022AAA.png' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2205.12134">
                    <papertitle>Adversarial Attack on Attackers: Post-Process to Mitigate Black-Box Score-Based Query Attacks</papertitle>
                </a>
                <br>

                <a href="https://scholar.google.com.hk/citations?user=X-JmE9UAAAAJ">Sizhe Chen</a>,
                <a href="">Zhehao Huang</a>,
                <a href="https://www.kuleuven.be/wieiswie/en/person/00140240">Qinghua Tao</a>,
                <a href="">Yingwen Wu</a>,
                <strong>Cihang Xie</strong>,
                <a href="https://scholar.google.com/citations?user=DR-gBcEAAAAJ">Xiaolin Huang</a>
                <br>
                <em>NeurIPS</em>, 2022
                <br>
                </p>
                <div class="paper" id="chen2022AAA">
                    <a href="https://arxiv.org/pdf/2205.12134.pdf">pdf</a> /
                    <a href="https://github.com/Sizhe-Chen/AAA">project page</a> /
                    <a href="data/chen2022AAA.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--chen2022AAA-->


        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/li2022videopretraining.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2205.01721">
                    <papertitle>In Defense of Image Pre-Training for Spatiotemporal Recognition</papertitle>
                </a>
                <br>
                <a href="https://xhl-video.github.io/xianhangli/">Xianhang Li</a>,
                <a href="https://csrhddlam.github.io/">Huiyu Wang</a>,
                <a href="https://weichen582.github.io/">Chen Wei</a>,
                <a href="https://meijieru.com/">Jieru Mei</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>,
                <strong>Cihang Xie</strong>
                <br>
                <em>ECCV</em>, 2022
                <br>
                </p>
                <div class="paper" id="li2022videopretraining">
                    <a href="https://arxiv.org/pdf/2205.01721.pdf">pdf</a> /
                    <a href="https://github.com/UCSC-VLAA/Image-Pretraining-for-Video">project page</a> /
                    <a href="data/li2022videopretraining.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--li2022videopretraining-->

        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/li2022vip.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7173_ECCV_2022_paper.php">
                    <papertitle>ViP: Unified Certified Detection and Recovery for Patch Attack with Vision Transformers</papertitle>
                </a>
                <br>
                <a href="https://ljb121002.github.io/">Junbo Li</a>,
                <a href="https://www.huan-zhang.com/">Huan Zhang</a>,
                <strong>Cihang Xie</strong>
                <br>
                <em>ECCV</em>, 2022
                <br>
                </p>
                <div class="paper" id="li2022vip">
                    <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136850566.pdf">pdf</a> /
                    <a href="data/li2022vip.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--li2022vip-->
        
        <tr>
          <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/ren2022sdmp.png' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2206.07692">
                    <papertitle>A Simple Data Mixing Prior for Improving Self-Supervised Learning</papertitle>
                </a>
                <br>
                <a href="https://oliverrensu.github.io/">Sucheng Ren</a>,
                <a href="https://csrhddlam.github.io/">Huiyu Wang</a>,
                <a href="https://zhengqigao.github.io/">Zhengqi Gao</a>,
                <a href="http://www.shengfenghe.com/">Shengfeng He</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>,
                <strong>Cihang Xie</strong>
                <br>
                <em>CVPR</em>, 2022
                <br>
                </p>
                <div class="paper" id="ren2022sdmp">
                    <a href="https://arxiv.org/pdf/2206.07692.pdf">pdf</a> /
                    <a href="https://github.com/OliverRensu/SDMP">project page</a> /
                    <a href="data/ren2022sdmp.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--ren2022sdmp-->

        <tr>
          <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/nataniel2021testface.png' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2106.04569">
                    <papertitle>Simulated Adversarial Testing of Face Recognition Models</papertitle>
                </a>
                <br>

                <a href="https://natanielruiz.github.io/">Nataniel Ruiz</a>,
                <a href="https://adamkortylewski.com/">Adam Kortylewski</a>,
                <a href="https://weichaoqiu.com/">Weichao Qiu</a>,
                <strong>Cihang Xie</strong>,
                <a href="https://cs-people.bu.edu/sbargal/">Sarah Adel Bargal</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                <a href="https://www.cs.bu.edu/fac/sclaroff/">Stan Sclaroff</a>
                <br>
                <em>CVPR</em>, 2022
                <br>
                </p>
                <div class="paper" id="nataniel2021testface">
                    <a href="https://arxiv.org/pdf/2106.04569.pdf">pdf</a> /
                    <a href="data/nataniel2021testface.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--nataniel2021testface-->

        


        <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/mei2022fastadvprop.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://openreview.net/forum?id=hcoswsDHNAW">
                    <papertitle>Fast AdvProp</papertitle>
                </a>
                <br>
                <a href="https://meijieru.com/">Jieru Mei</a>,
                <a href="https://tingxueronghua.github.io/">Yucheng Han</a>,
                <a href="https://scholar.google.com/citations?user=N1-l4GsAAAAJ">Yutong Bai</a>,
                <a href="https://scholar.google.com/citations?user=lU3wroMAAAAJ">Yixiao Zhang</a>,
                <a href="https://yingwei.li/">Yingwei Li</a>,
                <a href="https://xhl-video.github.io/xianhangli/">Xianhang Li</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                <strong>Cihang Xie</strong>              
                <br>
                <em>ICLR</em>, 2022
                <br>
                </p>
                <div class="paper" id="mei2022fastadvprop">
                    <a href="https://openreview.net/pdf?id=hcoswsDHNAW">pdf</a> /
                    <a href="https://github.com/meijieru/fast_advprop">project page</a> /
                    <a href="data/mei2022fastadvprop.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--mei2022fastadvprop-->

        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/zhou2021ibot.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2111.07832">
                    <papertitle>iBOT: Image BERT Pre-Training with Online Tokenizer</papertitle>
                </a>
                <br>
                <a href="https://shallowtoil.github.io/">Jinghao Zhou</a>,
                <a href="https://weichen582.github.io/">Chen Wei</a>,
                <a href="https://csrhddlam.github.io/">Huiyu Wang</a>,
                <a href="http://wei-shen.weebly.com/">Wei Shen</a>,
                <strong>Cihang Xie</strong>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                <a href="http://www.taokong.org/">Tao Kong</a>
                <br>
                <em>ICLR</em>, 2022
                <br>
                </p>
                <div class="paper" id="zhou2021ibot">
                    <a href="https://arxiv.org/pdf/2111.07832.pdf">pdf</a> /
                    <a href="https://github.com/bytedance/ibot">project page</a> /
                    <a href="https://medium.com/syncedreview/meet-ibot-a-masked-image-modelling-framework-that-enables-bert-like-pretraining-for-vision-da01002115e7">press</a> /
                    <a href="data/zhou2021ibot.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--zhou2021ibot-->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <h4><u>2021</u></h4>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>        


        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/bai2020vitsVScnns.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2111.05464">
                    <papertitle>Are Transformers More Robust Than CNNs?</papertitle>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=N1-l4GsAAAAJ">Yutong Bai</a>,
                <a href="https://meijieru.com/">Jieru Mei</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                <strong>Cihang Xie</strong>
                <br>
                <em>NeurIPS</em>, 2021
                <br>
                <font color="orange"><strong>The first benchmark that fairly compares Transformers with CNNs on robustness evaluations</strong></font>
                <br>
                </p>
                <div class="paper" id="bai2020vitsVScnns">
                    <a href="https://arxiv.org/pdf/2111.05464.pdf">pdf</a> /
                    <a href="https://github.com/ytongbai/ViTs-vs-CNNs">project page</a> /
                    <a href="data/bai2020vitsVScnns.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--bai2020vitsVScnns-->

        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/li2021calico.png' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2110.00519">
                    <papertitle>Calibrating Concepts and Operations: Towards Symbolic Reasoning on Real Images</papertitle>
                </a>
                <br>
                <a href="https://lizw14.github.io/">Zhuowan Li</a>,
                <a href="https://esteng.github.io/">Elias Stengel-Eskin</a>,
                <a href="https://scholar.google.com/citations?user=lU3wroMAAAAJ">Yixiao Zhang</a>,
                <strong>Cihang Xie</strong>,
                <a href="https://research.adobe.com/person/quan-hung-tran/">Quan Tran</a>,
                <a href="https://www.cs.jhu.edu/~vandurme/">Benjamin Van Durme</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                <br>
                <em>ICCV</em>, 2021
                <br>
                </p>
                <div class="paper" id="li2021calico">
                    <a href="https://arxiv.org/pdf/2110.00519.pdf">pdf</a> /
                    <a href="https://github.com/Lizw14/CaliCO">project page</a> /
                    <a href="data/li2021calico.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--li2021calico-->

        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/chen2021detadvprop.png' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2103.13886">
                    <papertitle>Robust and Accurate Object Detection via Adversarial Learning</papertitle>
                </a>
                <br>
                <a href="http://web.cs.ucla.edu/~xiangning/">Xiangning Chen</a>,
                <strong>Cihang Xie</strong>,
                <a href="https://scholar.google.com/citations?user=6POeyBoAAAAJ&hl=en">Mingxing Tan</a>,
                <a href="https://research.google/people/105588/">Li Zhang</a>,
                <a href="http://web.cs.ucla.edu/~chohsieh/">Cho-Jui Hsieh</a>,
                <a href="http://boqinggong.info/">Boqing Gong</a>
                <br>
                <em>CVPR</em>, 2021
                <br>
                </p>
                <div class="paper" id="chen2021detadvprop">
                    <a href="https://arxiv.org/pdf/2103.13886.pdf">pdf</a> /
                    <a href="https://github.com/google/automl/blob/master/efficientdet/Det-AdvProp.md">project page</a> /
                    <a href="data/chen2021detadvprop.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--chen2021detadvprop-->
        
        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/li2020shapetexture.jpg' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2010.05981">
                    <papertitle>Shape-Texture Debiased Neural Network Training</papertitle>
                </a>
                <br>
                <a href="https://yingwei.li/">Yingwei Li</a>,
                <a href="https://yucornetto.github.io/">Qihang Yu</a>,
                <a href="https://scholar.google.com/citations?user=6POeyBoAAAAJ&hl=en">Mingxing Tan</a>,
                <a href="https://meijieru.com/">Jieru Mei</a>,
                <a href="http://pengtang.xyz/">Peng Tang</a>,
                <a href="http://wei-shen.weebly.com/">Wei Shen</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                <strong>Cihang Xie</strong>
                <br>
                <em>ICLR</em>, 2021
                <br>
                </p>
                <div class="paper" id="li2020shapetexture">
                    <a href="https://arxiv.org/pdf/2010.05981.pdf">pdf</a> /
                    <a href="https://github.com/LiYingwei/ShapeTextureDebiasedTraining">project page</a> /
                    <a href="data/li2020shapetexture.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--li2020shapetexture-->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <h4><u>2020</u></h4>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 

        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/JHU.jpg' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://jscholarship.library.jhu.edu/handle/1774.2/63790">
                    <papertitle>Towards Robust Representation Learning and Beyond</papertitle>
                </a>
                <br>
                <strong>Cihang Xie</strong>
                <br>
                <em>PhD Dissertation</em>
                <br>
                </p>
                <div class="paper" id="xie2020phd">
                    <a href="data/phd_thesis.pdf">pdf</a> /
                    <a href="data/xie2020phd.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--xie2020phd-->

        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/xie2020sat.jpg' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2006.14536">
                    <papertitle>Smooth Adversarial Training</papertitle>
                </a>
                <br>
                <strong>Cihang Xie</strong>,
                <a href="https://scholar.google.com/citations?user=6POeyBoAAAAJ&hl=en">Mingxing Tan</a>,
                <a href="http://boqinggong.info/">Boqing Gong</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                <a href="https://cs.stanford.edu/~quocle/">Quoc Le</a>
                <br>
                <em>Tech report</em>, arXiv
                <br>
                <font color="orange"><strong>State-of-the-art method for defending against adversarial attacks on ImageNet</strong></font>
                <br>
                </p>
                <div class="paper" id="xie2020sat">
                    <a href="https://arxiv.org/pdf/2006.14536.pdf">pdf</a> /
                    <a href="https://github.com/cihangxie/SmoothAdversarialTraining">project page</a> /
                    <a href="data/xie2020sat.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--xie2020sat-->

        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/yang2020patchattack.png' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2004.05682">
                    <papertitle>PatchAttack: A Black-box Texture-based Attack with Reinforcement Learning</papertitle>
                </a>
                <br>
                <a href="https://chenglin-yang.github.io/">Chenglin Yang</a>,
                <a href="https://adamkortylewski.com/">Adam Kortylewski</a>,
                <strong>Cihang Xie</strong>,
                <a href="https://www.yinzhicao.org/">Yinzhi Cao</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                <br>
                <em>ECCV</em>, 2020
                <br>
                </p>
                <div class="paper" id="yang2020patchattack">
                    <a href="https://arxiv.org/pdf/2004.05682.pdf">pdf</a> /
                    <a href="https://github.com/Chenglin-Yang/PatchAttack">project page</a> /
                    <a href="data/yang2020patchattack.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--yang2020patchattack-->
        
        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/li2019regional.png' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/1904.00979">
                    <papertitle>Regional Homogeneity: Towards Learning Transferable Universal Adversarial Perturbations Against Defenses</papertitle>
                </a>
                <br>
                <a href="https://yingwei.li/">Yingwei Li</a>,
                <a href="http://songbai.site/">Song Bai</a>,
                <strong>Cihang Xie</strong>,
                <a href="https://www.researchgate.net/profile/Zhenyu_Liao4">Zhenyu Liao</a>,
                <a href="http://users.eecs.northwestern.edu/~xsh835/">Xiaohui Shen</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                <br>
                <em>ECCV</em>, 2020
                <br>
                </p>
                <div class="paper" id="li2019regional">
                    <a href="https://arxiv.org/pdf/1904.00979.pdf">pdf</a> /
                    <a href="https://github.com/LiYingwei/Regional-Homogeneity">project page</a> /
                    <a href="data/li2019regional.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--li2019regional-->
            
        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/xie2020advprop.png' width="250"></div>
            </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/1911.09665">
                    <papertitle>Adversarial Examples Improve Image Recognition</papertitle>
                </a>
                <br>
                <strong>Cihang Xie</strong>,
                <a href="https://scholar.google.com/citations?user=6POeyBoAAAAJ&hl=en">Mingxing Tan</a>,
                <a href="http://boqinggong.info/">Boqing Gong</a>,
                <a href="http://wangjiangb.github.io/">Jiang Wang</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                <a href="https://cs.stanford.edu/~quocle/">Quoc Le</a>
                <br>
                <em>CVPR</em>, 2020
                <br>
                <font color="orange"><strong>State-of-the-art ImageNet classifier without extra training data</strong></font>
                <br>
                </p>
                <div class="paper" id="xie2020advprop">
                    <a href="https://arxiv.org/pdf/1911.09665.pdf">pdf</a> /
                    <a href="https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet">project page</a> /
                    <a href="https://github.com/tingxueronghua/pytorch-classification-advprop">pytorch reimplementation</a> /
                    <a href="data/AdvProp_Talk.pdf">slides</a> /
                    <a href="https://medium.com/syncedreview/google-johns-hopkins-university-can-adversarial-examples-improve-image-recognition-bcb7254e2d8">press</a> /
                    <a href="https://www.youtube.com/watch?v=KTCztkNJm50">video</a> /
                    <a href="data/xie2020advprop.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--xie2020advprop-->
        
        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/li2020nas.png' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2004.01961">
                    <papertitle>Neural Architecture Search for Lightweight Non-Local Networks</papertitle>
                </a>
                <br>
                <a href="https://yingwei.li/">Yingwei Li</a>,
                <a href="https://scholar.google.com/citations?user=OEZ816YAAAAJ">Xiaojie Jin</a>,
                <a href="https://meijieru.com/">Jieru Mei</a>,
                <a href="https://scholar.google.com/citations?user=-jNTDU0AAAAJ&hl">Xiaochen Lian</a>,
                <a href="https://sites.google.com/site/linjieyang89/">Linjie Yang</a>,
                <strong>Cihang Xie</strong>,
                <a href="https://yucornetto.github.io/">Qihang Yu</a>,
                <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>,
                <a href="http://songbai.site/">Song Bai</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                <br>
                <em>CVPR</em>, 2020
                <br>
                </p>
                <div class="paper" id="li2020nas">
                    <a href="https://arxiv.org/pdf/2004.01961.pdf">pdf</a> /
                    <a href="https://github.com/LiYingwei/AutoNL">project page</a> /
                    <a href="data/li2020nas.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--li2020nas-->
  
        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/huang2019upc.gif' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/1909.04326">
                    <papertitle>Universal Physical Camouflage Attacks on Object Detectors</papertitle>
                </a>
                <br>
                <a href="">Lifeng Huang</a>,
                <a href="http://sdcs.sysu.edu.cn/content/2537">Chengying Gao</a>,
                <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>,
                <strong>Cihang Xie</strong>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                <a href="https://changqingzou.weebly.com/">Changqing Zou</a>,
                <a href="http://sdcs.sysu.edu.cn/node/2495">Ning Liu</a>
                <br>
                <em>CVPR</em>, 2020
                <br>
                </p>
                <div class="paper" id="huang2019upc">
                    <a href="https://arxiv.org/pdf/1909.04326.pdf">pdf</a> /
                    <a href="https://mesunhlf.github.io/index_physical.html">project page</a> /
                    <a href="data/huang2019upc.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--huang2019upc-->
                        
        <tr>
          <td style="padding:20px;width:35%;vertical-align:middle">
              <img src='images/xie2020intriguing.png' width="250"></div>
          </td>
          <td width="75%" valign="middle">
            <p>
              <a href="https://arxiv.org/abs/1906.03787">
                <papertitle>Intriguing Properties of Adversarial Training at Scale</papertitle>
              </a>
              <br>
              <strong>Cihang Xie</strong>,
              <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
              <br>
              <em>ICLR</em>, 2020
              <br>
            </p>
            <div class="paper" id="xie2020intriguing">
              <a href="https://arxiv.org/pdf/1906.03787.pdf">pdf</a> /
              <a href="data/xie2020intriguing.bib">bibtex</a>
            </div>
            <br>
          </td>
        </tr> <!--xie2020intriguing-->
        
        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/li2020learning.png' width="250"></div>
            </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/1812.03413">
                    <papertitle>Learning Transferable Adversarial Examples via Ghost Networks</papertitle>
                </a>
                <br>
                <a href="https://yingwei.li/">Yingwei Li</a>,
                <a href="http://songbai.site/">Song Bai</a>,
                <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>,
                <strong>Cihang Xie</strong>,
                <a href="https://zhishuai.xyz/">Zhishuai Zhang</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                <br>
                <em>AAAI</em>, 2020
                <br>
                </p>
                <div class="paper" id="li2020learning">
                    <a href="https://arxiv.org/pdf/1812.03413.pdf">pdf</a> /
                    <a href="https://github.com/LiYingwei/ghost-network">project page</a> /
                    <a href="data/li2020learning.bib">bibtex</a>
                </div>
                <br>
              </td>
          </tr> <!--li2020learning-->


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <h4><u>2019</u></h4>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 

        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/xie2019denoising.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/1812.03411">
                    <papertitle>Feature Denoising for Improving Adversarial Robustness</papertitle>
                </a>
                <br>
                <strong>Cihang Xie</strong>,
                <a href="http://ppwwyyxx.com/">Yuxin Wu</a>,
                <a href="https://lvdmaaten.github.io/">Laurens van der Maaten</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
                <a href="http://kaiminghe.com/">Kaiming He</a>
                <br>
                <em>CVPR</em>, 2019
                <br>
                <font color="orange"><strong>The first ImageNet classifier that can successfully defend against strong white-box adversarial attacks</strong></font>
                <br>
                </p>
                <div class="paper" id="xie2019denoising">
                    <a href="https://arxiv.org/pdf/1812.03411.pdf">pdf</a> /
                    <a href="https://github.com/facebookresearch/ImageNet-Adversarial-Training">project page</a> /
                    <a href="data/Feature_Denoise.pdf">slides</a> /
                    <a href="https://ai.facebook.com/blog/feature-denoising/">blog</a> /
                    <a href="data/xie2019denoising.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--xie2019denoising-->
        
        <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/xie2019DiverseInput.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/1803.06978">
                    <papertitle>Improving Transferability of Adversarial Examples with Input Diversity</papertitle>
                </a>
                <br>
                <strong>Cihang Xie</strong>,
                <a href="https://zhishuai.xyz/">Zhishuai Zhang</a>,
                <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>,
                <a href="http://songbai.site/">Song Bai</a>,
                <a href="http://www.jianyuwang.org/">Jianyu Wang</a>,
                <a href="http://web.cs.ucla.edu/~zhou.ren/">Zhou Ren</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                <br>
                <em>CVPR</em>, 2019
                <br>
                </p>
                <div class="paper" id="xie2019DiverseInput">
                    <a href="https://arxiv.org/pdf/1803.06978.pdf">pdf</a> /
                    <a href="https://github.com/cihangxie/DI-2-FGSM">project page</a> /
                    <a href="data/xie2019DiverseInput.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--xie2019DiverseInput-->


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <h4><u>2018</u></h4>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
        
        <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/xie2018randomization.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/1711.01991">
                    <papertitle>Mitigating Adversarial Effects Through Randomization</papertitle>
                </a>
                <br>
                <strong>Cihang Xie</strong>,
                <a href="http://www.jianyuwang.org/">Jianyu Wang</a>,
                <a href="https://zhishuai.xyz/">Zhishuai Zhang</a>,
                <a href="http://web.cs.ucla.edu/~zhou.ren/">Zhou Ren</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                <br>
                <em>ICLR</em>, 2018
                <br>
                <font color="orange"><strong>The runner-up solution in the adversarial defense track of NIPS 2017 Adversarial Attacks and Defenses Competition</strong></font>
                <br>
                </p>
                <div class="paper" id="xie2018randomization">
                    <a href="https://arxiv.org/pdf/1711.01991.pdf">pdf</a> /
                    <a href="https://github.com/cihangxie/NIPS2017_adv_challenge_defense">project page</a> /
                    <a href="https://arxiv.org/pdf/1804.00097.pdf">competition book</a> /
                    <a href="data/NIPS_ADV.pdf">slides</a> /
                    <a href="data/xie2018randomization.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--xie2018randomization-->
        
        <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/zhang2018des.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/1712.00433">
                    <papertitle>Single-Shot Object Detection with Enriched Semantics</papertitle>
                </a>
                <br>
                <a href="https://zhishuai.xyz/">Zhishuai Zhang</a>,
                <a href="https://www.cs.jhu.edu/~syqiao/">Siyuan Qiao</a>,
                <strong>Cihang Xie</strong>,
                <a href="http://wei-shen.weebly.com/">Wei Shen</a>,
                <a href="https://vectorinstitute.ai/team/bo-wang/">Bo Wang</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                <br>
                <em>CVPR</em>, 2018
                <br>
                </p>
                <div class="paper" id="zhang2018des">
                    <a href="https://arxiv.org/pdf/1712.00433.pdf">pdf</a> /
                    <a href="https://github.com/bairdzhang/des">project page</a> /
                    <a href="data/zhang2018des.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--zhang2018des-->
        
        <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/zhang2018deepvoting.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/1709.04577">
                    <papertitle>DeepVoting: A Robust and Explainable Deep Network for Semantic Part Detection under Partial Occlusion</papertitle>
                </a>
                <br>
                <a href="https://zhishuai.xyz/">Zhishuai Zhang</a>,
                <strong>Cihang Xie</strong>,
                <a href="http://www.jianyuwang.org/">Jianyu Wang</a>,
                <a href="http://lingxixie.com/Home.html">Lingxi Xie</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                <br>
                <em>CVPR</em>, 2018
                <br>
                </p>
                <div class="paper" id="zhang2018deepvoting">
                    <a href="https://arxiv.org/pdf/1709.04577.pdf">pdf</a> /
                    <a href="data/zhang2018deepvoting.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--zhang2018deepvoting-->
        
        <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <div class="one">
                    <div class="two" id='wang2018vcsp'><img src='images/penguin.png' width="250"></div>
                    <img src='images/monkey.png' width="250">
                    </div>
                <script type="text/javascript">
                    function portrait_start() {
                        document.getElementById('wang2018vcsp').style.opacity = "1";
                    }
                
                function portrait_stop() {
                    document.getElementById('wang2018vcsp').style.opacity = "0";
                }
                portrait_stop()
                </script>
            </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/1711.04451">
                    <papertitle>Visual Concepts and Compositional Voting</papertitle>
                </a>
                <br>
                <a href="http://www.jianyuwang.org/">Jianyu Wang</a>,
                <a href="https://zhishuai.xyz/">Zhishuai Zhang</a>,
                <strong>Cihang Xie</strong>,
                <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>,
                <a href="http://vittalp.github.io/">Vittal Premachandran</a>,
                <a href="https://scholar.google.com/citations?user=4e5ajP4AAAAJ&hl=en">Jun Zhu</a>,
                <a href="http://lingxixie.com/Home.html">Lingxi Xie</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                <br>
                <em>Annals of Mathematical Sciences and Applications</em>, 2018
                <br>
                </p>
                <div class="paper" id="wang2018vcsp">
                    <a href="https://arxiv.org/pdf/1711.04451.pdf">pdf</a> /
                    <a href="data/wang2018vcsp.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--wang2018vcsp-->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <h4><u>2017</u></h4>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
        
        <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/xie2017dag.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/1703.08603">
                    <papertitle>Adversarial Examples for Semantic Segmentation and Object Detection</papertitle>
                </a>
                <br>
                <strong>Cihang Xie</strong>,
                <a href="http://www.jianyuwang.org/">Jianyu Wang</a>,
                <a href="https://zhishuai.xyz/">Zhishuai Zhang</a>,
                <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>,
                <a href="http://lingxixie.com/Home.html">Lingxi Xie</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                <br>
                <em>ICCV</em>, 2017
                <br>
                </p>
                <div class="paper" id="xie2017dag">
                    <a href="https://arxiv.org/pdf/1703.08603.pdf">pdf</a> /
                    <a href="https://github.com/cihangxie/DAG">project page</a> /
                    <a href="data/DAG.pdf">slides</a> /
                    <a href="data/xie2017dag.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--xie2017dag-->
        
        <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/wang2017voting.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/1707.07819">
                    <papertitle>Detecting Semantic Parts on Partially Occluded Objects</papertitle>
                </a>
                <br>
                <a href="http://www.jianyuwang.org/">Jianyu Wang</a>,
                <strong>Cihang Xie</strong>,
                <a href="https://zhishuai.xyz/">Zhishuai Zhang</a>,
                <a href="https://scholar.google.com/citations?user=4e5ajP4AAAAJ&hl=en">Jun Zhu</a>,
                <a href="http://lingxixie.com/Home.html">Lingxi Xie</a>,
                <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
                <br>
                <em>BMVC</em>, 2017
                <br>
                </p>
                <div class="paper" id="wang2017voting">
                    <a href="https://arxiv.org/pdf/1707.07819.pdf">pdf</a> /
                    <a href="data/wang2017voting.bib">bibtex</a>
                </div>
                <br>
            </td>
        </tr> <!--wang2017voting-->
        
        </tbody></table>
        <hr>
        
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>Open Sources</heading>
                </td>
            </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        
        <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/cleverhans.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://github.com/tensorflow/cleverhans">
                    <papertitle>CleverHans Adversarial Examples Library</papertitle>
                </a>
                <br>
                </p>
                <div class="paper" id="cleverhans">
                    <a href="https://github.com/tensorflow/cleverhans">project page</a> /
                    <a href="https://arxiv.org/pdf/1610.00768.pdf">tech report</a>
                </div>
            </td>
        </tr> <!--cleverhans-->
        
        <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/faceattack.jpg' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://github.com/ppwwyyxx/Adversarial-Face-Attack">
                    <papertitle>Adversarial Attacks on Face Recognition</papertitle>
                </a>
                <br>
                </p>
                <div class="paper" id="cleverhans">
                    <a href="https://github.com/ppwwyyxx/Adversarial-Face-Attack">project page</a> /
                    <a href="https://www.jiqizhixin.com/articles/2018-10-26-11">press (chinese)</a>
                </div>
            </td>
        </tr> <!--face attack-->
        
        </tbody></table>
        <hr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td width=50% align="center">
              <a href="https://clustrmaps.com/site/1b2nv"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=_s2_OduoO888ugvX-pYCsciaAlSir2c84N9WH4N3AmQ&cl=ffffff" /></a>
            </td>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;">Stolen from <a href="https://jonbarron.info/">Jon Barron</a></p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>

<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>

<!-- Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-131560165-1', 'auto');
  ga('send', 'pageview');
</script>
<!-- End Google Analytics -->

</body>

</html>
